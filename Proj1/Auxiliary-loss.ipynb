{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper *Self-Supervised Generalisation with Meta Auxiliary Learning*\n",
    "\n",
    "https://papers.nips.cc/paper/2019/file/92262bf907af914b95a0fc33c3f33bf6-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an auxiliary loss is called auxiliary learning, and is similar to multi-task learning, except that only the primary task is of interest. In our case, the primary task is to output whether image 1 is a digit smaller than image 2. The role of the auxiliary task is to assist in generalization of this primary task.\n",
    "\n",
    "This paper discusses unsupervised auxiliary learning, for the cases when no label exist for the auxiliary task. Note: this is not our case, we have the original labels $\\{0, 1, \\dots, 9\\}$ from which we deduce the boolean value. Also, the project presentation PDF mentions:\n",
    "\n",
    "> For the [auxiliary loss], the training can in particular take advantage of the availability of the classes of the two digits in each pair, beside the Boolean value truly of interest. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google paper \"Going deeper with convolutions\"\n",
    "\n",
    "https://arxiv.org/abs/1409.4842\n",
    "\n",
    "https://stats.stackexchange.com/questions/304699/what-is-auxiliary-loss-as-mentioned-in-pspnet-paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper from Google introduced the concept of auxiliary loss. But the focus is to improve very deep networks (they describe a 22-layer long network), whereas we don't really have deep networks for our simple task. The basic idea is to add **auxiliary classifiers connected to intermediate layers**. This is especially useful to **fight gradient vanishing and add regularization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of extra network\n",
    "\n",
    "1. Average pooling layer\n",
    "1. 1x1 convolution for dimension reduction and relu activation\n",
    "1. fully connected and relu\n",
    "1. dropout (70%)\n",
    "1. linear layer with softmax loss as classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft ideas for Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this [medium article](https://towardsdatascience.com/improve-your-neural-networks-generalization-performance-by-adding-an-unsupervised-auxiliary-loss-4d58b2dead54), we can combine the main loss with the auxiliary loss for the total loss:\n",
    "\n",
    "$$\\text{total loss} = \\text{main loss} + \\lambda \\cdot \\text{auxiliary loss}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since our network is *not* deep (and can't be, because computations must be rather fast as mentionned in the project description), we should probably put the auxiliary classifier at the end of the network, and not at intermediate levels (as done in Google paper). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we must combine the main and auxiliary loss in a meaningful way:\n",
    "\n",
    "* Main loss is currently an accuracy \n",
    "* Auxiliary loss would probably be categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably change the main loss to something more related to the multi-class entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LaNz2YYhwbXx"
   },
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torchvision import datasets\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from torch.utils.data import TensorDataset, DataLoader\r\n",
    "\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAzBfBHS0yjp"
   },
   "source": [
    "## dataset build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsfGSYY9QaKA",
    "outputId": "3e29e18e-7160-4f4b-8cac-c4f32fc09f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-08 21:38:53--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
      "--2021-03-08 21:38:53--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: ‘MNIST.tar.gz.1’\n",
      "\n",
      "MNIST.tar.gz.1          [        <=>         ]  13.79M   591KB/s               ^C\n"
     ]
    }
   ],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6_TIp4ACQhpr"
   },
   "outputs": [],
   "source": [
    "!tar -xf MNIST.tar.gz\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adfc8yshRCUe",
    "outputId": "01e3407b-189f-4e4d-d60b-96459ad8ee1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.pt  training.pt\n"
     ]
    }
   ],
   "source": [
    "!ls MNIST/processed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e6oc9g2JyCEw"
   },
   "outputs": [],
   "source": [
    "######################################################################\r\n",
    "\r\n",
    "def mnist_to_pairs(nb, input, target):\r\n",
    "    input = torch.functional.F.avg_pool2d(input, kernel_size = 2)\r\n",
    "    a = torch.randperm(input.size(0))\r\n",
    "    a = a[:2 * nb].view(nb, 2)\r\n",
    "    input = torch.cat((input[a[:, 0]], input[a[:, 1]]), 1)\r\n",
    "    classes = target[a]\r\n",
    "    target = (classes[:, 0] <= classes[:, 1]).long()\r\n",
    "    return input, target, classes\r\n",
    "\r\n",
    "######################################################################\r\n",
    "\r\n",
    "def generate_pair_sets(nb):\r\n",
    "\r\n",
    "    train_set = datasets.MNIST('', train = True, download = True)\r\n",
    "    train_input = train_set.data.view(-1, 1, 28, 28).float()\r\n",
    "    train_target = train_set.targets\r\n",
    "\r\n",
    "    test_set = datasets.MNIST('', train = False, download = True)\r\n",
    "    test_input = test_set.data.view(-1, 1, 28, 28).float()\r\n",
    "    test_target = test_set.targets\r\n",
    "\r\n",
    "    return mnist_to_pairs(nb, train_input, train_target) + \\\r\n",
    "           mnist_to_pairs(nb, test_input, test_target)\r\n",
    "\r\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "av161kLTFpWj"
   },
   "outputs": [],
   "source": [
    "train_input , train_target , train_classes , test_input , test_target , test_classes = generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "XGi49cC5F1T3",
    "outputId": "1cd17e02-2132-4fec-9d77-eab2cceddb0a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape : torch.Size([1000, 2, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "plt.imshow( train_input[0][0].numpy() ) \r\n",
    "plt.show()\r\n",
    "plt.imshow( train_input[0][1].numpy() ) \r\n",
    "plt.show()\r\n",
    "print(f'train shape : {train_input.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ImsRSpztHZ_Z"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input,train_target , train_classes)\r\n",
    "train_dataloader = DataLoader(train_dataset , batch_size=100)\r\n",
    "\r\n",
    "test_dataset = TensorDataset(test_input,test_target , test_classes)\r\n",
    "test_dataloader = DataLoader(test_dataset , batch_size=100)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully dense net with aux loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6NDo4PotHZye"
   },
   "outputs": [],
   "source": [
    "class FullyDenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyDenseNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2*14*14, 300)\n",
    "        self.fc2 = nn.Linear(300, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)\n",
    "        self.fc4 = nn.Linear(100, 50)\n",
    "        self.fc5 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten(1)(x)\n",
    "        x = torch.relu( self.fc1(x) )\n",
    "        x = torch.relu( self.fc2(x) )\n",
    "        x = torch.relu( self.fc3(x) )\n",
    "        x = torch.relu( self.fc4(x) )\n",
    "        x = self.fc5(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6NDo4PotHZye"
   },
   "outputs": [],
   "source": [
    "class FullyDenseNetAux(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyDenseNetAux, self).__init__()\n",
    "\n",
    "        # Network basis: common for all losses\n",
    "        # 14 * 14 = 196\n",
    "        self.fc1_im1 = nn.Linear(14*14, 100)\n",
    "        self.fc1_im2 = nn.Linear(14*14, 100)\n",
    "        \n",
    "        self.fc2_im1 = nn.Linear(100, 50)\n",
    "        self.fc2_im2 = nn.Linear(100, 50)\n",
    "        \n",
    "        # Auxiliary networks\n",
    "        self.fc3_im1 = nn.Linear(50, 10)\n",
    "        self.fc3_im2 = nn.Linear(50, 10)\n",
    "        \n",
    "        # Main task\n",
    "        self.fc4 = nn.Linear(2*50, 10)\n",
    "        self.fc5 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        im1 = nn.Flatten()(x[:, 0, :, :])\n",
    "        im2 = nn.Flatten()(x[:, 1, :, :])\n",
    "        \n",
    "        im1 = torch.relu(self.fc1_im1(im1))\n",
    "        im2 = torch.relu(self.fc1_im2(im2))\n",
    "        \n",
    "        im1 = torch.relu(self.fc2_im1(im1))\n",
    "        im2 = torch.relu(self.fc2_im2(im2))\n",
    "        \n",
    "        # Main task\n",
    "        common = torch.cat((im1, im2), dim=1)\n",
    "        common = torch.relu(self.fc4(common))\n",
    "        common = self.fc5(common)\n",
    "        common = F.sigmoid(common)\n",
    "        \n",
    "        # Auxiliary networks\n",
    "        im1 = self.fc3_im1(im1)\n",
    "        im1 = F.softmax(im1)\n",
    "        \n",
    "        im2 = self.fc3_im2(im2)\n",
    "        im2 = F.softmax(im2)\n",
    "        \n",
    "        return common, im1, im2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6589189767837524 50.91 tensor(0.5844) 48.552\n",
      "1 0.48521560430526733 50.8760009765625 tensor(0.5688) 48.688\n",
      "2 0.4298025667667389 51.042001953125 tensor(0.5805) 48.174\n",
      "3 0.3910297751426697 51.032001953125 tensor(0.5940) 47.886\n",
      "4 0.36674779653549194 50.912001953125 tensor(0.5776) 48.224\n",
      "5 0.37001627683639526 50.6360009765625 tensor(0.5107) 49.242\n",
      "6 0.31918391585350037 50.557998046875 tensor(0.4855) 52.356\n",
      "7 0.2548154890537262 50.6660009765625 tensor(0.4833) 51.894\n",
      "8 0.19821672141551971 50.767998046875 tensor(0.4985) 51.838\n",
      "9 0.1679983139038086 50.792001953125 tensor(0.5085) 51.48\n",
      "10 0.14728650450706482 50.882001953125 tensor(0.5195) 50.72\n",
      "11 0.12207827717065811 50.6760009765625 tensor(0.6308) 53.526\n",
      "12 0.19551147520542145 50.4660009765625 tensor(0.6134) 53.193999999999996\n",
      "13 0.14905351400375366 51.027998046875 tensor(0.6278) 53.122\n",
      "14 0.11941385269165039 50.667998046875 tensor(0.5406) 51.824\n",
      "15 0.13974426686763763 50.9160009765625 tensor(0.5548) 49.45\n",
      "16 0.13733993470668793 50.8460009765625 tensor(0.6100) 48.77\n",
      "17 0.0744851753115654 50.752001953125 tensor(0.7297) 53.812\n",
      "18 0.058474160730838776 50.797998046875 tensor(0.8641) 54.29\n",
      "19 0.05140538886189461 50.8239990234375 tensor(0.8276) 53.888000000000005\n",
      "20 0.0790279358625412 50.92 tensor(0.6226) 52.562\n",
      "21 0.14044587314128876 50.722001953125 tensor(0.5948) 53.066\n",
      "22 0.04573604464530945 50.857998046875 tensor(0.5934) 52.228\n",
      "23 0.02198568359017372 50.8139990234375 tensor(0.5936) 51.13\n",
      "24 0.009299185127019882 50.8160009765625 tensor(0.6325) 50.928000000000004\n",
      "25 0.006973314099013805 50.8160009765625 tensor(0.6580) 50.928000000000004\n",
      "26 0.005498533137142658 50.8160009765625 tensor(0.6778) 50.912\n",
      "27 0.004536784254014492 50.8160009765625 tensor(0.6938) 50.943999999999996\n",
      "28 0.0038618866819888353 50.8160009765625 tensor(0.7075) 50.94\n",
      "29 0.003350795479491353 50.8160009765625 tensor(0.7197) 50.94\n"
     ]
    }
   ],
   "source": [
    "net = FullyDenseNet()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "tot_train_loss , tot_train_acc , tot_test_loss , tot_test_acc = train(net, optimizer, F.binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maousi/anaconda3/envs/ml/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "<ipython-input-11-c0597ce55c3e>:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  im1 = F.softmax(im1)\n",
      "<ipython-input-11-c0597ce55c3e>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  im2 = F.softmax(im2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3471938669681549 53.2 tensor(0.1102) 56.5\n",
      "1 -0.07583669573068619 52.3739990234375 tensor(-0.1400) 52.86\n",
      "2 -0.2540977895259857 51.43 tensor(-0.2573) 50.922\n",
      "3 -0.3904423117637634 50.8960009765625 tensor(-0.3181) 49.898\n",
      "4 -0.4854885935783386 50.7860009765625 tensor(-0.3574) 50.07\n",
      "5 -0.5614346265792847 50.767998046875 tensor(-0.4032) 50.692\n",
      "6 -0.6354878544807434 50.887998046875 tensor(-0.4307) 50.268\n",
      "7 -0.7142586708068848 50.917998046875 tensor(-0.4408) 50.552\n",
      "8 -0.7655872106552124 50.93 tensor(-0.4552) 49.972\n",
      "9 -0.8210858106613159 50.96 tensor(-0.5064) 51.076\n",
      "10 -0.8529136776924133 50.91 tensor(-0.5105) 51.258\n",
      "11 -0.8885366320610046 51.06 tensor(-0.4908) 50.496\n",
      "12 -0.9068744778633118 50.852001953125 tensor(-0.5312) 51.614\n",
      "13 -0.7646325826644897 50.787998046875 tensor(-0.5017) 53.354\n",
      "14 -0.8853691816329956 51.047998046875 tensor(-0.6560) 51.618\n",
      "15 -1.0521290302276611 50.89 tensor(-0.6639) 51.244\n",
      "16 -1.1075416803359985 50.887998046875 tensor(-0.6694) 51.342\n",
      "17 -1.138146162033081 50.8160009765625 tensor(-0.6435) 50.556000000000004\n",
      "18 -1.1550319194793701 50.8039990234375 tensor(-0.6429) 50.681999999999995\n",
      "19 -1.1085014343261719 50.937998046875 tensor(-0.6572) 51.006\n",
      "20 -1.1415607929229736 50.8939990234375 tensor(-0.1131) 47.168\n",
      "21 -1.1536283493041992 50.72 tensor(-0.6064) 52.84\n",
      "22 -1.1839786767959595 50.542001953125 tensor(-0.5352) 53.346000000000004\n",
      "23 -1.2148669958114624 50.81 tensor(-0.5653) 52.97\n",
      "24 -1.2340166568756104 50.6760009765625 tensor(-0.5786) 52.596000000000004\n",
      "25 -1.2948808670043945 50.802001953125 tensor(-0.5974) 53.044\n",
      "26 -1.2524441480636597 50.8139990234375 tensor(-0.2973) 47.663999999999994\n",
      "27 -1.2899768352508545 50.587998046875 tensor(-0.7606) 52.416000000000004\n",
      "28 -1.3616690635681152 50.832001953125 tensor(-0.7346) 52.356\n",
      "29 -1.3788238763809204 50.8639990234375 tensor(-0.7238) 52.408\n"
     ]
    }
   ],
   "source": [
    "net = FullyDenseNetAux()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "tot_train_loss , tot_train_acc , tot_test_loss , tot_test_acc = train(net, optimizer, criterion_=custom_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA3Ui4BnShQu"
   },
   "source": [
    "# Train tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, target, classes):\n",
    "    main, im1, im2 = output\n",
    "    \n",
    "    main_loss = F.binary_cross_entropy(main.flatten(), target)\n",
    "    aux_loss_1 = F.nll_loss(im1, classes[:, 0])\n",
    "    aux_loss_2 = F.nll_loss(im2, classes[:, 1])\n",
    "    \n",
    "    return main_loss + aux_loss_1 + aux_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_loss(criterion):\n",
    "  if criterion is not custom_loss:\n",
    "    internal_criterion = lambda output, target, _: criterion(output.flatten(), target)\n",
    "    compute_acc = lambda output, target: (target == torch.round(output) ).sum().item()\n",
    "  else:\n",
    "    internal_criterion = criterion\n",
    "    compute_acc = lambda output, target: (target == torch.round(output[0]) ).sum().item()\n",
    "    \n",
    "  return internal_criterion, compute_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i54w5hBmHaES"
   },
   "outputs": [],
   "source": [
    "def train_epoch(network_ , optimizer_ , criterion = F.binary_cross_entropy ):\n",
    "  internal_criterion, compute_acc = handle_loss(criterion)\n",
    "    \n",
    "  loss_tot = []\n",
    "  acc_tot = []\n",
    "  network_.train()\n",
    "  for batch_idx, (data, target , classes ) in enumerate(train_dataloader):\n",
    "    optimizer_.zero_grad()\n",
    "    output = network_(data)\n",
    "    loss = internal_criterion(output, target.to(torch.float32), classes)\n",
    "    loss.backward()\n",
    "    optimizer_.step()\n",
    "\n",
    "    loss_tot.append(loss.item())\n",
    "    acc_tot.append( compute_acc(output, target) )\n",
    "  \n",
    "  return torch.FloatTensor(loss_tot).mean().item() , torch.FloatTensor(acc_tot).mean().item()/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xe7whYlPQoLi"
   },
   "outputs": [],
   "source": [
    "def test(network_ , criterion_ = F.binary_cross_entropy):\n",
    "  internal_criterion, compute_acc = handle_loss(criterion_)\n",
    "\n",
    "  network_.eval()\n",
    "  test_loss = 0\n",
    "  acc = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data, target , classes in test_dataloader:\n",
    "      output = network_(data)\n",
    "      test_loss += internal_criterion(output, target.to(torch.float32), classes)\n",
    "      acc += compute_acc(output, target)\n",
    "\n",
    "  test_loss /= len(test_dataloader)\n",
    "  acc /= len(test_dataloader)\n",
    "  return test_loss , acc/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-c0597ce55c3e>:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  im1 = F.softmax(im1)\n",
      "<ipython-input-11-c0597ce55c3e>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  im2 = F.softmax(im2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.25306418538093567 48.442001953125 tensor(-0.0231) 50.222\n",
      "1 -0.19650132954120636 50.53 tensor(-0.1727) 48.681999999999995\n",
      "2 -0.35344260931015015 50.7739990234375 tensor(-0.2374) 47.854\n",
      "3 -0.45223158597946167 50.5960009765625 tensor(-0.3300) 49.508\n",
      "4 -0.5171807408332825 50.502001953125 tensor(-0.3838) 49.23\n",
      "5 -0.6004834175109863 50.6 tensor(-0.4790) 50.532\n",
      "6 -0.6957592964172363 50.522001953125 tensor(-0.5611) 51.27\n",
      "7 -0.8166763186454773 50.5460009765625 tensor(-0.5250) 52.626000000000005\n",
      "8 -0.8450201153755188 50.6560009765625 tensor(-0.5720) 51.382\n",
      "9 -0.8971213102340698 50.7060009765625 tensor(-0.5629) 51.044\n",
      "10 -0.9311823844909668 50.647998046875 tensor(-0.5630) 51.251999999999995\n",
      "11 -0.963970959186554 50.7460009765625 tensor(-0.4387) 49.523999999999994\n",
      "12 -0.9470165371894836 50.7839990234375 tensor(-0.5763) 50.083999999999996\n",
      "13 -0.9613487124443054 50.602001953125 tensor(-0.5541) 50.238\n",
      "14 -1.0090757608413696 50.6339990234375 tensor(-0.5136) 52.95399999999999\n",
      "15 -0.9839712381362915 50.532001953125 tensor(-0.4952) 52.58\n",
      "16 -1.06119966506958 50.7060009765625 tensor(-0.4423) 52.84\n",
      "17 -1.0750175714492798 50.7239990234375 tensor(-0.4056) 52.946000000000005\n",
      "18 -1.0870001316070557 50.757998046875 tensor(-0.4389) 52.61600000000001\n",
      "19 -1.0880229473114014 50.8060009765625 tensor(-0.4095) 52.49\n",
      "20 -1.094697117805481 50.85 tensor(-0.4066) 52.238\n",
      "21 -0.9371591806411743 51.12 tensor(-0.5200) 50.722\n",
      "22 -0.9214426875114441 50.547998046875 tensor(-0.3040) 53.048\n",
      "23 -1.0674843788146973 50.72 tensor(-0.4227) 52.416000000000004\n",
      "24 -1.0762498378753662 50.7939990234375 tensor(-0.4672) 52.44\n",
      "25 -1.0916606187820435 50.767998046875 tensor(-0.0789) 54.008\n",
      "26 -1.099919319152832 50.84 tensor(-0.4316) 52.256\n",
      "27 -1.1130821704864502 50.98 tensor(-0.4510) 51.228\n",
      "28 -1.103208303451538 50.962001953125 tensor(-0.5116) 51.483999999999995\n",
      "29 -1.1091803312301636 51.0239990234375 tensor(-0.5645) 51.608000000000004\n"
     ]
    }
   ],
   "source": [
    "net = FullyDenseNetAux()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "tot_train_loss , tot_train_acc , tot_test_loss , tot_test_acc = train(net, optimizer, criterion_=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "MHcA1vKgHaGi"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-42-d5b1ccc08e0a>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-d5b1ccc08e0a>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    passt_loss , test_acc)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def train(network_ , optimizer_ , criterion_ = F.binary_cross_entropy ,epoch_nb = 30,   debug_ = True):\r\n",
    "\r\n",
    "  tot_train_loss = []\r\n",
    "  tot_train_acc = []\r\n",
    "  tot_test_loss = []\r\n",
    "  tot_test_acc = []\r\n",
    "\r\n",
    "\r\n",
    "  for epoch in range(epoch_nb):\r\n",
    "    train_loss , train_acc = train_epoch(network_ , optimizer_ , criterion_)\r\n",
    "    test_loss , test_acc = test(network_ , criterion_)\r\n",
    "\r\n",
    "    tot_train_loss.append(train_loss)\r\n",
    "    tot_train_acc.append(train_acc)\r\n",
    "    tot_test_loss.append(test_loss)\r\n",
    "    tot_test_acc.append(test_acc)\r\n",
    "\n",
    "        passt_loss , test_acc)\r\n",
    "\r\n",
    "  return tot_train_loss , tot_train_acc , tot_test_loss , tot_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYg-E2UB8Flj"
   },
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ffB574WB8IFx"
   },
   "outputs": [],
   "source": [
    "def train_multiple_runs( network_class , runs = 10 , epoch = 30):\r\n",
    "  all_train_loss , all_train_acc , all_test_loss , all_test_acc = [],[],[],[]\r\n",
    "  \r\n",
    "  for i in range(runs):\r\n",
    "    n = network_class()\r\n",
    "    optimizer = optim.SGD(n.parameters(), lr=0.01, momentum=0.5)\r\n",
    "    criterion = F.binary_cross_entropy\r\n",
    "\r\n",
    "    tot_train_loss , tot_train_acc , tot_test_loss , tot_test_acc = train(n , optimizer , criterion , epoch , debug_ = False)\r\n",
    "    all_train_loss.append(tot_train_loss)\r\n",
    "    all_train_acc.append(tot_train_acc)\r\n",
    "    all_test_loss.append(tot_test_loss)\r\n",
    "    all_test_acc.append(tot_test_acc)\r\n",
    "\r\n",
    "  return all_train_loss , all_train_acc , all_test_loss , all_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C66k1nu4HaMp"
   },
   "outputs": [],
   "source": [
    "def plot_loss_acc( tot_train_loss , tot_train_acc , tot_test_loss , tot_test_acc ):\r\n",
    "  epochs = range(1, len(tot_train_loss)+1 )\r\n",
    "  plt.plot(epochs, tot_train_loss, 'g', label='Training loss')\r\n",
    "  plt.plot(epochs, tot_test_loss, 'b', label='Test loss')\r\n",
    "  plt.plot(epochs, tot_train_acc, 'r', label='Training acc')\r\n",
    "  plt.plot(epochs, tot_test_acc, 'y', label='Test acc')\r\n",
    "  plt.title('Training and Test loss/acc')\r\n",
    "  plt.xlabel('Epochs')\r\n",
    "  plt.ylabel('loss/acc')\r\n",
    "  plt.ylim((0,1))\r\n",
    "  plt.legend()\r\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fi4wxxDM9KWH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "def plot_std_loss_acc(all_train_loss , all_train_acc , all_test_loss , all_test_acc):\r\n",
    "  trl_mean = np.array(all_train_loss).mean(axis = 0)\r\n",
    "  tel_mean = np.array(all_train_acc).mean(axis = 0)\r\n",
    "  tra_mean = np.array(all_test_loss).mean(axis = 0)\r\n",
    "  tea_mean = np.array(all_test_acc).mean(axis = 0)\r\n",
    "\r\n",
    "  trl_std = np.array(all_train_loss).std(axis = 0)\r\n",
    "  tel_std = np.array(all_train_acc).std(axis = 0)\r\n",
    "  tra_std = np.array(all_test_loss).std(axis = 0)\r\n",
    "  tea_std = np.array(all_test_acc).std(axis = 0)\r\n",
    "\r\n",
    "  epochs = range(1, len(tea_std)+1 )\r\n",
    "\r\n",
    "  temp = [ [trl_mean , trl_std] , [tel_mean , tel_std] , [tra_mean , tra_std] , [tea_mean , tea_std] ]\r\n",
    "\r\n",
    "  for g in temp : \r\n",
    "    plt.plot( epochs , g[0] )\r\n",
    "    plt.fill_between(epochs, g[0]-g[1], g[0]+g[1] ,alpha=0.3)\r\n",
    "\r\n",
    "  plt.ylim((-0.1,1.1))\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
